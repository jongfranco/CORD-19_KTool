{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from ipywidgets import Layout, Button, Box, FloatProgress\n",
    "from ipyfilechooser import FileChooser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from file_builder.research_aspect_classifier import ResearchAspectClassifier\n",
    "\n",
    "import IPython\n",
    "store = IPython.get_ipython().find_line_magic('store')\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "import scispacy\n",
    "from scispacy.umls_linking import UmlsEntityLinker\n",
    "\n",
    "import ktrain\n",
    "import csv\n",
    "\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_uids = ['wtvfow2f','jja2wcfz','d22fs0fx','36h7zzlo','klv9jdm8','ujnspv4j','n0uzbda0','s1hj89kj','0tpmz9uy','11minoss','bk601l5a','v6jyplcn','b5u5hp2r','6bq0tps2','hgpgeel6','puvdw0ci','uym826bh','0qaoam29','8qp8o2g2','xwzichc3','t3cpzmwj','jeglryus','z2lv280w','stqj3ue5','fc338qdt', '0084poiq', '00cef9ee', 'puc13jf1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACRO_TOPICS = ['Coronavirus', 'Public health and epidemics', 'Molecular biology', 'Influenza', 'Immunology',\n",
    "               'Rotavirus', 'Antivirals', 'Clinical trials', 'Testing and diagnosing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESEARCH_ASPECTS = [\"Background\", \"Purpose\", \"Method\", \"Finding_Contribution\", \"Other\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "av = [\"Background\", \"Purpose\", \"Method\", \"Finding_Contribution\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sentences = spacy.load('en_core_sci_lg')\n",
    "linker = UmlsEntityLinker(resolve_abbreviations=True)\n",
    "nlp_sentences.add_pipe(linker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_sentences('Bulbo-Spinal Atrophy')\n",
    "entity = doc.ents[0]\n",
    "\n",
    "print(linker.umls.cui_to_entity[entity._.umls_ents[0][0]].concept_id)\n",
    "print(linker.umls.cui_to_entity[entity._.umls_ents[0][0]].canonical_name)\n",
    "\n",
    "\n",
    "#for umls_ent in entity._.umls_ents:\n",
    "#    print(linker.umls.cui_to_entity[umls_ent[0]].canonical_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON builder and topic builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Builder:\n",
    "    def __init__(self, metadata, progress_bar):\n",
    "        self.metadata = metadata\n",
    "        self.progress_bar = progress_bar\n",
    "        self.entries = {}\n",
    "        self.nlp_sentences = spacy.load('en_core_sci_lg')\n",
    "        self.zsl = ktrain.text.ZeroShotClassifier()\n",
    "        #self.linker = EntityLinker(resolve_abbreviations=True, name=\"umls\")\n",
    "        \n",
    "        self.rac = ResearchAspectClassifier(RESEARCH_ASPECTS)\n",
    "        progress_bar.value += 25\n",
    "        \n",
    "    def return_dict_uids(self, uids=None):\n",
    "        reader = csv.DictReader(self.metadata)\n",
    "        papers = {}\n",
    "\n",
    "        for row in reader:\n",
    "            if(uids is not None):\n",
    "                if(row['cord_uid'] in uids):\n",
    "                    papers[row['cord_uid']] = {'title': row['title'], 'abstract':row['abstract'], 'publish_time':row['publish_time']}\n",
    "            else:\n",
    "                papers[row['cord_uid']] = {'title': row['title'], 'abstract':row['abstract'], 'publish_time':row['publish_time']}\n",
    "        \n",
    "        print(len(papers))\n",
    "        return papers\n",
    "        \n",
    "        \n",
    "    def return_docs_dict(self, papers):\n",
    "         \n",
    "        progress_paper = 75.0/float(len(papers))\n",
    "                                \n",
    "        for i, (uid, paper) in enumerate(papers.items()):  \n",
    "            try:\n",
    "                abstract_nlp = self.nlp_sentences(paper['abstract'])\n",
    "                sentences = [sent.text.strip() for sent in abstract_nlp.sents]\n",
    "\n",
    "                classified_abstract = self.rac.classify_abstract(sentences)\n",
    "\n",
    "\n",
    "                is_covid = bool(re.search('COVID-19', paper['abstract'], re.IGNORECASE) or re.search('COVID', paper['abstract'], re.IGNORECASE))\n",
    "                ents = [ent.text for ent in abstract_nlp.ents]\n",
    "                umls_ents = []\n",
    "                \n",
    "                for ent in ents:\n",
    "                    umls_ents.append({'id': linker.umls.cui_to_entity[ent._.umls_ents[0][0]].concept_id, 'name':linker.umls.cui_to_entity[ent._.umls_ents[0][0]].canonical_name})\n",
    "\n",
    "                #if(len(sentences) > 0):\n",
    "                #    macro_topic = self.zsl.predict(sentences, topic_strings=MACRO_TOPICS, include_labels=True)[0][0] \n",
    "                #else:\n",
    "                #    macro_topic = \"\"\n",
    "\n",
    "                self.entries[uid] = {'title': paper['title'],'abstract': classified_abstract, 'is_covid': is_covid, 'umls_ents': umls_ents, 'publish_time': paper['publish_time']}\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            \n",
    "            progress_bar.value += progress_paper\n",
    "\n",
    "        print('== Done Building Docs Dictionary ==')\n",
    "        return self.entries\n",
    "    \n",
    "    def return_topic_models(self, docs_dict, aspects_to_train):\n",
    "        topic_models = {'covid_papers':{}, 'all_papers':{}}\n",
    "        texts = {'covid_papers':{}, 'all_papers':{}}\n",
    "        uids = {'covid_papers':[], 'all_papers':[]}\n",
    "        build_docs = {'covid_papers':{}, 'all_papers':{}}\n",
    "        \n",
    "        for aspect in aspects_to_train:\n",
    "            texts['covid_papers'][aspect] = []\n",
    "            texts['all_papers'][aspect] = []\n",
    "            \n",
    "            build_docs['covid_papers'][aspect] = []\n",
    "            build_docs['all_papers'][aspect] = []\n",
    "            \n",
    "        \n",
    "        for i, (uid, doc) in enumerate(docs_dict.items()):  \n",
    "            if(doc['is_covid']):\n",
    "                uids['covid_papers'].append(uid)\n",
    "            uids['all_papers'].append(uid)\n",
    "            \n",
    "            for aspect in aspects_to_train:\n",
    "                if(aspect == 'Whole'):\n",
    "                    whole_text_dict = {}\n",
    "                    whole_text = ''\n",
    "                    for aspect in RESEARCH_ASPECTS:\n",
    "                        for sentence in doc['abstract'][aspect]:\n",
    "                            whole_text_dict[sentence[0]] = sentence[1]\n",
    "\n",
    "                    for key in sorted(whole_text_dict):\n",
    "                        whole_text+= whole_text_dict[key]\n",
    "\n",
    "                    if(doc['is_covid']):\n",
    "                        texts['covid_papers']['Whole'].append(whole_text)\n",
    "\n",
    "                    texts['all_papers']['Whole'].append(whole_text)\n",
    "                else:\n",
    "                    if(doc['is_covid']):\n",
    "                        texts['covid_papers'][aspect].append(' '.join([sentence[1] for sentence in doc['abstract'][aspect]]))\n",
    "\n",
    "                    texts['all_papers'][aspect].append(' '.join([sentence[1] for sentence in doc['abstract'][aspect]]))\n",
    "                \n",
    "        for i, (classification, aspects) in enumerate(texts.items()):  \n",
    "            for z, (aspect, docs) in enumerate(aspects.items()):  \n",
    "                try:\n",
    "                    topic_models[classification][aspect] = ktrain.text.get_topic_model(docs, min_df=0, lda_max_iter=1, verbose=False, n_features=30000)\n",
    "                    build_docs[classification][aspect] = docs\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                                            \n",
    "        return topic_models, uids, build_docs\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipdir(path, ziph):\n",
    "    # ziph is zipfile handle\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            ziph.write(os.path.join(root, file))\n",
    "\n",
    "def create_dir(path):\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % path)\n",
    "    \n",
    "\n",
    "def build_files(metadata, progress_bar, aspects, is_test):\n",
    "    builder = Builder(metadata, progress_bar)\n",
    "    \n",
    "    create_dir('built_files')\n",
    "    \n",
    "    if(is_test):\n",
    "        docs_dict = builder.return_docs_dict(builder.return_dict_uids(test_uids))\n",
    "    else:\n",
    "        docs_dict = builder.return_docs_dict(builder.return_dict_uids())\n",
    "    \n",
    "    #uncomment\n",
    "    #docs_dict = {}\n",
    "    #with zipfile.ZipFile('built_files_cord19ktool.zip') as zf:\n",
    "    #    with zf.open('built_files/docs_dict.p') as docs:\n",
    "    #        docs_dict = pickle.load( docs )        \n",
    "        \n",
    "    topic_models, uids, build_docs = builder.return_topic_models(docs_dict, aspects)\n",
    "    \n",
    "    with open('built_files/docs_dict.p', 'wb') as fp:\n",
    "        pickle.dump(docs_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    \n",
    "    for i, (classification, aspects) in enumerate(topic_models.items()):  \n",
    "        create_dir('built_files/'+classification)\n",
    "        \n",
    "        with open('built_files/'+classification+'/uids.p', 'wb') as fp:\n",
    "            pickle.dump(uids[classification], fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        for z, (aspect, model) in enumerate(aspects.items()):  \n",
    "            print(aspect)\n",
    "            create_dir('built_files/'+classification+'/'+aspect)\n",
    "            model.save('built_files/'+classification+'/'+aspect+'/'+aspect)\n",
    "            \n",
    "            with open('built_files/'+classification+'/'+aspect+'/docs.p', 'wb') as fp:\n",
    "                pickle.dump(build_docs[classification][aspect], fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "    zipf = zipfile.ZipFile('built_files_cord19ktool.zip', 'w', zipfile.ZIP_DEFLATED)\n",
    "    zipdir('built_files', zipf)\n",
    "    zipf.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP JSON FILE FROM METADATA.CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RESEARCH_ASPECTS.extend(['Whole'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def enable_btns_setup_server():\n",
    "    btn_setup_server.disabled = False\n",
    "    btn_setup_server_test.disabled = False\n",
    "    \n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "title = widgets.Label(value='Upload the metadata.csv from CODA-19 to build files')\n",
    "\n",
    "aspects = widgets.SelectMultiple(\n",
    "    options=RESEARCH_ASPECTS,\n",
    "    description='Research Aspects to train',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "btn_setup_server = widgets.Button(\n",
    "    description='BUILD FILES',\n",
    "    disabled=True,\n",
    "    button_style='', \n",
    "    icon='gear' \n",
    ")\n",
    "\n",
    "btn_setup_server_test = widgets.Button(\n",
    "    description='BUILD FILES FOR TEST PAPERS',\n",
    "    disabled=True,\n",
    "    button_style='', \n",
    "    icon='gear' \n",
    ")\n",
    "\n",
    "progress_bar = FloatProgress(min=0, max=100) \n",
    "\n",
    "def selected_file(fc):\n",
    "    enable_btns_setup_server()\n",
    "\n",
    "def pressed_build_files(btn):\n",
    "    display(progress_bar)\n",
    "    with open(fc.selected_path+'/'+fc.selected_filename) as cord19:\n",
    "        build_files(cord19, progress_bar, aspects.value, False)\n",
    "    \n",
    "    cord19.close()\n",
    "    \n",
    "def pressed_build_files_test(btn):\n",
    "    display(progress_bar)\n",
    "    with open(fc.selected_path+'/'+fc.selected_filename) as cord19:\n",
    "        build_files(cord19, progress_bar, aspects.value, True)\n",
    "    \n",
    "    cord19.close()\n",
    "    \n",
    "fc = FileChooser('./')\n",
    "display(fc)\n",
    "fc.register_callback(selected_file)\n",
    "\n",
    "\n",
    "btn_setup_server.on_click(pressed_build_files)\n",
    "btn_setup_server_test.on_click(pressed_build_files_test)\n",
    "\n",
    "#display(title)\n",
    "display(aspects)\n",
    "display(btn_setup_server)\n",
    "display(btn_setup_server_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
